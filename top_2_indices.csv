index,id,text,gold,summary,id_candidate
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","###Summary### This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain.",0
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",This is achieved by designing an adversarial reconstruction network.,1
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network.,2
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space.",3
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network.",4
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin.,5
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain.,6
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",The paper performs experiments on several domain adaptation tasks on digit datasets.,7
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc.",8
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc.",9
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",to illustrate the effectiveness of the proposed approach.,10
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","### Novelty ###The model proposed in this paper is extended from the domain adversarial training approach.",11
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","To stabilize the gradient, the model replaces the domain classifier with a reconstruction network.",12
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","In this way, the discriminator only discriminates the reconstructed data from the source domain.",13
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",This idea is interesting and provides some novelty.,14
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","###Clarity###Overall, the paper is well organized and logically clear.",15
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",The claims are well-supported by the experiments.,16
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",The images are well-presented and well-explained by the captions and the text.,17
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","###Pros###1) The paper proposes a Max-margin based approach to tackle domain adaptation.",18
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin.",19
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",The idea is interesting and heuristic to the domain adaptation research community.,20
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones.,21
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",3) The paper provides many analyses to demonstrate the effectiveness of the proposed method.,22
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","###Cons###1) The experimental part of this paper is weak.",23
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach.",24
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",Further experimental results on image recognition or NLP task is desired.,25
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/2) The organization and presentation of this paper should be polished.",26
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","Based on the summary, cons, and pros, the current rating I am giving now is weak reject.",27
0,https://openreview.net/forum?id=BklEF3VFPB,"###Summary###-----This paper proposes Max-margin domain adversarial training (MDAT) to tackle the problem of transferring knowledge from a rich-labeled source domain to an unlabeled target domain. This is achieved by designing an adversarial reconstruction network. The proposed MDAT stabilizes the gradient by replacing the domain classifier with a reconstruction network. ----------The motivation of the proposed network is based on the observations that the traditional domain-adversarial training is vulnerable in the following aspects:1) the training procedure of the domain discriminator is unstable, 2) it only considers the feature-level alignment, 3) it lacks the interpretable explanation for the learned feature space. ----------In the proposed method, the Adversarial Reconstruction Network (ARN) consists of a shared feature extractor, a label predictor, and a reconstruction network. The reconstruction network only focuses on reconstructing samples on the source domain and pushing the target domain away from a margin. The feature extractor tries to confuse the decoder by learning to reconstruct samples on the target domain. ----------The paper performs experiments on several domain adaptation tasks on digit datasets. The experimental results demonstrate the effectiveness of the proposed results over several baselines such as DANN, ADDA, CyCADA, CADA, etc. ----------The paper also provides empirical analyses such as t-SNE embedding, plotting the loss, etc. to illustrate the effectiveness of the proposed approach. ----------### Novelty ###----------The model proposed in this paper is extended from the domain adversarial training approach. To stabilize the gradient, the model replaces the domain classifier with a reconstruction network. In this way, the discriminator only discriminates the reconstructed data from the source domain. This idea is interesting and provides some novelty.  ----------###Clarity###----------Overall, the paper is well organized and logically clear. The claims are well-supported by the experiments. The images are well-presented and well-explained by the captions and the text. ----------###Pros###----------1) The paper proposes a Max-margin based approach to tackle domain adaptation. Instead of leveraging the domain discriminator to discriminate the source from the target, this paper utilizes a reconstructor to push the target domain far away from the margin. The idea is interesting and heuristic to the domain adaptation research community. -----2) The experimental results on digit benchmark demonstrate the effectiveness of the proposed method over other baselines including the most state-of-the-art ones. ----------3) The paper provides many analyses to demonstrate the effectiveness of the proposed method. ----------###Cons###---------------1) The experimental part of this paper is weak. The paper only provides experimental results on the digit recognition experiments, which is not enough to demonstrate the effectiveness and robustness of the proposed approach. Further experimental results on image recognition or NLP task is desired. ----------It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:-----DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019. http://ai.bu.edu/DomainNet/-----Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017. http://hemanthdv.org/OfficeHome-Dataset/-----2) The organization and presentation of this paper should be polished.----------Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","I would like to discuss the final rating with other reviewers, ACs.",28
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation.",0
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose.",1
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",The method is very similar to some of the existing works in the literature.,2
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.,3
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","Pros:- The writing is good- Satisfactory empirical results Cons:- The proposed method is very similar to certain methods in the literature Detail comments: (1) The proposed loss function Eq.",4
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",(8) is very similar to the contrastive loss proposed by Hadsell et al.,5
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","(2006, Eq.",6
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","(4)), which is used in Siamese GAN variants (Juefei-Xu et al.",7
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","2018, Hsu et al.",8
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",2019).,9
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",Thus essentially the proposed method is an application of an existing GAN technique.,10
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",Its novelty is limited.,11
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","(2) Experiments - How are the hyperparameters selected?",12
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",It is essential to specify the selection criteria when labeled target data is not available.,13
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",- What does the * in DRCN* mean in Table 1?,14
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",- ARN w.o.,15
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore.,16
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.,17
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",(3) In Eq.,18
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.",19
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","Typos:- In Eq.",20
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","(1), there is a missing D in the first term.",21
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",D_s should be \mathcal{D}_s to match previous notation.,22
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",- In Eq.,23
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","(2), the ""0,"" is not meaningful given the definition of []^+.",24
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","Refs- Hadsell, R., Chopra, S. and LeCun, Y., 2006, June.",25
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",Dimensionality reduction by learning an invariant mapping.,26
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol.,27
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","2, pp.",28
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",1735-1742).,29
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",IEEE.,30
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","- Juefei-Xu, F., Dey, R., Boddeti, V.N.",31
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","and Savvides, M., 2018.",32
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",RankGAN: A Maximum Margin Ranking GAN for Generating Faces.,33
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",In Asian Conference on Computer Vision (pp.,34
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",3-18).,35
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","Springer, Cham.",36
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","- Hsu, C.C., Lin, C.W., Su, W.T.",37
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","and Cheung, G., 2019.",38
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",SiGAN: Siamese generative adversarial network for identity-preserving face hallucination.,39
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","IEEE Transactions on Image Processing, 28(12), pp.6225-6236.",40
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","# Update after rebuttal Thank you for the response and additional experiment results.",41
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated.",42
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.",It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN.,43
1,https://openreview.net/forum?id=BklEF3VFPB,"This work proposes Adversarial Reconstruction Network (ARN), a network architecture, and Max-margin Domain-Adversarial Training (MDAT), an objective and training procedure for unsupervised domain adaptation. Similar to domain adversarial approaches, the generator aims at finding domain invariant representation while the discriminator now monitors the reconstruction loss of the source and target data using hinge-like lose. The method is very similar to some of the existing works in the literature. Experiment results on the standard digit datasets and the WiFi gesture recognition dataset show that the proposed method outperforms other alternatives.----------Pros:------ The writing is good------ Satisfactory empirical results----------Cons:------ The proposed method is very similar to certain methods in the literature----------Detail comments:-----(1) The proposed loss function Eq.(8) is very similar to the contrastive loss proposed by Hadsell et al. (2006, Eq.(4)), which is used in Siamese GAN variants (Juefei-Xu et al. 2018, Hsu et al. 2019). Thus essentially the proposed method is an application of an existing GAN technique. Its novelty is limited.----------(2) Experiments------ How are the hyperparameters selected? It is essential to specify the selection criteria when labeled target data is not available.------ What does the * in DRCN* mean in Table 1?------ ARN w.o. MDAT may not be the best alternative since the target data is ignored in the reconstruction and the discriminator is not discriminating anymore. A more reasonable alternative would be to ignore the margin and minimize L_r(x^s)-L_r(x^t) to see the effect of the margin.----------(3) In Eq.(2), y is said to be the predicted domain label (-1 or +1), which could be not accurate according to the common hinge loss definition.----------Typos:------ In Eq.(1), there is a missing D in the first term. D_s should be \mathcal{D}_s to match previous notation.------ In Eq.(2), the ""0,"" is not meaningful given the definition of []^+.----------Refs------ Hadsell, R., Chopra, S. and LeCun, Y., 2006, June. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) (Vol. 2, pp. 1735-1742). IEEE.------ Juefei-Xu, F., Dey, R., Boddeti, V.N. and Savvides, M., 2018. RankGAN: A Maximum Margin Ranking GAN for Generating Faces. In Asian Conference on Computer Vision (pp. 3-18). Springer, Cham.------ Hsu, C.C., Lin, C.W., Su, W.T. and Cheung, G., 2019. SiGAN: Siamese generative adversarial network for identity-preserving face hallucination. IEEE Transactions on Image Processing, 28(12), pp.6225-6236.----------# Update after rebuttal----------Thank you for the response and additional experiment results. I agree that MDAT and SiGAN are not using the contrastive loss in the same way, but claiming that they are ""totally different"" can be misleading and overstated. It would be better if the paper includes proper discussion about the contrastive loss from the literature and distinguish the particularities between MDAT and SiGAN. Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.","This paper proposes max-margin domain adversarial training with an adversarial reconstruction network that stabilizes the gradient by replacing the domain classifier.----------Reviewers and AC think that the method is interesting and motivation is reasonable. Concerns were raised regarding weak experimental results in the diversity of datasets and the comparison to state-of-the-art methods. The paper needs to show how the method works with respect to stability and interpretability. The paper should also clearly relate the contrastive loss for reconstruction to previous work, given that both the loss and the reconstruction idea have been extensively explored for DA. Finally, the theoretical analysis is shallow and the gap between the theory and the algorithm needs to be closed.----------Overall this is a borderline paper. Considering the bar of ICLR and limited quota, I recommend rejection.","Overall, I think the proposed method shows some prosperity thus I have increased my score accordingly.",44
