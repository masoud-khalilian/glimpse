{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r3g2uNZkWiM"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/masoud-khalilian/glimpse.git\n",
        "!pwd\n",
        "%cd /content/glimpse/\n",
        "!pwd\n",
        "!pip install torch==2.5.1+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install pandas\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install tqdm\n",
        "!pip install transformers\n",
        "# !pip install numpy==1.25.2\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install gradio\n",
        "!pip install pandas\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install SentencePiece\n",
        "!python ./glimpse/data_loading/data_processing.py\n",
        "\n",
        "import nltk\n",
        "# Download the required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data\n",
        "\n",
        "# Example usage\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello, how are you?\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BGdmIWxLrCk"
      },
      "outputs": [],
      "source": [
        "!python ./glimpse/data_loading/data_processing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxiiXqA8kva6"
      },
      "outputs": [],
      "source": [
        "!python abstractive.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQoyRdS_k0IA"
      },
      "outputs": [],
      "source": [
        "!python extractive.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7BVUqDnd10v"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azd_3EIK1Pn5"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score\n",
        "!pip install lexrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lpQDXuIzq-a"
      },
      "outputs": [],
      "source": [
        "import pickle as pk\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "\n",
        "from lexrank import LexRank\n",
        "from lexrank.mappings.stopwords import STOPWORDS\n",
        "import nltk\n",
        "\n",
        "path = Path(\"./output/\")\n",
        "output_path =  Path(\"output/summaries/methods_reviews/\")\n",
        "\n",
        "def consensus_scores_based_summaries(sample, n_consensus=3, n_dissensus=3):\n",
        "    consensus_samples = sample['consensuality_scores'].sort_values(ascending=True).head(n_consensus).index.tolist()\n",
        "    disensus_samples = sample['consensuality_scores'].sort_values(ascending=False).head(n_dissensus).index.tolist()\n",
        "\n",
        "    consensus = \".\".join(consensus_samples)\n",
        "    disensus = \".\".join(disensus_samples)\n",
        "\n",
        "    return consensus + \"\\n\\n\" + disensus\n",
        "\n",
        "\n",
        "def rsa_scores_based_summaries(sample, n_consensus=3, n_rsa_speaker=3):\n",
        "    consensus_samples = sample['consensuality_scores'].sort_values(ascending=True).head(n_consensus).index.tolist()\n",
        "    rsa = sample['best_rsa'].tolist()[:n_rsa_speaker]\n",
        "\n",
        "    consensus = \".\".join(consensus_samples)\n",
        "    rsa = \".\".join(rsa)\n",
        "\n",
        "    return consensus + \"\\n\\n\" + rsa\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def lead(sample, N=10):\n",
        "    texts = sample['speaker_df'].index.tolist()\n",
        "\n",
        "    summary = \"\\n\".join([\".\".join(t.split('.')[:N]) for t in texts])\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "def construct_templated_summaries(data, fn, dataset=None):\n",
        "    records = []\n",
        "    for sample in data['results']:\n",
        "        summary = fn(sample)\n",
        "        text = \"\\n\\n\".join(sample['speaker_df'].index.tolist())\n",
        "        record = {'id' : sample['id'], 'summary': summary, 'metadata/reranking_model' : data['metadata/reranking_model'], 'metadata/rsa_iterations' : data['metadata/reranking_model'], \"text\": text}\n",
        "        if dataset is not None:\n",
        "            record['gold'] = dataset.loc[sample[\"id\"]]['gold'].tolist()[0]\n",
        "            if record['gold'] is not None:\n",
        "                rouges = scorer.score(summary, record['gold'])\n",
        "                record |= {r : v.fmeasure  for r, v in rouges.items()}\n",
        "\n",
        "\n",
        "\n",
        "        records.append(record)\n",
        "\n",
        "    return pd.DataFrame.from_records(records)\n",
        "\n",
        "def prepare_dataset(dataset_name, dataset_path=\"data/processed/\"):\n",
        "    print(\"prepare_dataset\")\n",
        "    print(dataset_name)\n",
        "    dataset_path = Path(dataset_path)\n",
        "    if dataset_name == \"amazon\":\n",
        "        dataset = pd.read_csv(dataset_path / \"amazon_test.csv\")\n",
        "    elif dataset_name == \"space\":\n",
        "        dataset = pd.read_csv(dataset_path / \"space.csv\")\n",
        "    elif dataset_name == \"yelp\":\n",
        "        dataset = pd.read_csv(dataset_path / \"yelp_test.csv\")\n",
        "    elif dataset_name == \"reviews\":\n",
        "        dataset = pd.read_csv(dataset_path / \"test_metareviews.csv\")\n",
        "    elif dataset_name == \"all_reviews_2020\":\n",
        "        dataset = pd.read_csv(dataset_path / \"all_reviews_2020.csv\")\n",
        "    elif dataset_name == \"all_reviews_2018\":\n",
        "        dataset = pd.read_csv(dataset_path / \"all_reviews_2018.csv\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset {dataset_name}\")\n",
        "\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "for n in [1, 2,3,4]:\n",
        "    for file in path.glob(\"*.pk\"):\n",
        "        print(file)\n",
        "        with file.open('rb') as fd:\n",
        "            data = pk.load(fd)\n",
        "\n",
        "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "        # print(str(file.stem).split('-_-'))\n",
        "        model_name, dataset_name, decoding_config, date = str(file.stem).split('-_-')[:4]\n",
        "\n",
        "        # parts = file.stem.split('_')\n",
        "        # model_name = parts[0]  # 'extractive_sentences'\n",
        "        # dataset_name = parts[2]  # 'all_reviews'\n",
        "        # decoding_config = parts[3]  # '2021_none'\n",
        "        # date = '-_-'.join(parts[4:])  # '\n",
        "\n",
        "        print(dataset_name)\n",
        "        print(dataset_name)\n",
        "        print(dataset_name)\n",
        "\n",
        "        dataset = prepare_dataset(dataset_name, dataset_path=\"./data/processed/\")\n",
        "        dataset = dataset.set_index('id')\n",
        "\n",
        "        fn = lambda sample: consensus_scores_based_summaries(sample, n_consensus=n, n_dissensus=n)\n",
        "\n",
        "        df = construct_templated_summaries(data, fn, dataset=dataset)\n",
        "\n",
        "        df['metadata/method'] = \"Agreement\"\n",
        "        df['metadata/n_sentences'] = 2*n\n",
        "        df['metadata/n_consensus'] = n\n",
        "        df['metadata/n_dissensus'] = n\n",
        "\n",
        "        name = file.stem + \"-_-\" + f\"consensus_score_based_{n}.csv\"\n",
        "\n",
        "        if (output_path / name).exists():\n",
        "            df_old = pd.read_csv(output_path / name)\n",
        "\n",
        "            for col in df.columns:\n",
        "                if col not in df_old.columns:\n",
        "                    df_old[col] = float(\"nan\")\n",
        "\n",
        "            # add entry to the dataframe\n",
        "            for col in df.columns:\n",
        "                df_old[col] = df[col]\n",
        "\n",
        "            df = df_old\n",
        "\n",
        "        df.to_csv(output_path / name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for n in [1, 2, 3, 4, 5, 6]:\n",
        "#     for file in path.glob(\"*.pk\"):\n",
        "#         with file.open('rb') as fd:\n",
        "#             data = pk.load(fd)\n",
        "\n",
        "#         Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "#         model_name, dataset_name, decoding_config, date = str(file.stem).split('-_-')[:4]\n",
        "#         parts = file.stem.split('_')\n",
        "#         model_name = parts[0]  # 'extractive_sentences'\n",
        "#         dataset_name = parts[2]  # 'all_reviews'\n",
        "#         decoding_config = parts[3]  # '2021_none'\n",
        "#         date = '_'.join(parts[4:])  # '\n",
        "#         dataset = prepare_dataset(dataset_name, dataset_path=\"./data/processed/\")\n",
        "#         dataset = dataset.set_index('id')\n",
        "\n",
        "#         fn = lambda sample: rsa_scores_based_summaries(sample, n_consensus=n, n_rsa_speaker=n)\n",
        "#         df = construct_templated_summaries(data, fn, dataset=dataset)\n",
        "\n",
        "#         df['metadata/method'] = \"Speaker+Agreement\"\n",
        "#         df['metadata/n_sentences'] = 2*n\n",
        "#         df['metadata/n_consensus'] = n\n",
        "#         df['metadata/n_dissensus'] = n\n",
        "\n",
        "#         name = file.stem + \"-_-\" + f\"rsa_score_based_{n}.csv\"\n",
        "\n",
        "#         if (output_path / name).exists():\n",
        "#             df_old = pd.read_csv(output_path / name)\n",
        "\n",
        "#             for col in df.columns:\n",
        "#                 if col not in df_old.columns:\n",
        "#                     df_old[col] = float(\"nan\")\n",
        "\n",
        "#             # add entry to the dataframe\n",
        "#             for col in df.columns:\n",
        "#                 df_old[col] = df[col]\n",
        "\n",
        "#             df = df_old\n",
        "\n",
        "#         df.to_csv(output_path / name)\n",
        "\n",
        "# for n in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
        "#     print(n,\"in loop\")\n",
        "#     for file in path.glob(\"*.pk\"):\n",
        "#         print(file)\n",
        "#         with file.open('rb') as fd:\n",
        "#             data = pk.load(fd)\n",
        "\n",
        "#         Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "#         parts = file.stem.split('_')\n",
        "#         model_name = parts[0]  # 'extractive_sentences'\n",
        "#         dataset_name = parts[2]  # 'all_reviews'\n",
        "#         decoding_config = parts[3]  # '2021_none'\n",
        "#         date = '_'.join(parts[4:])  # '\n",
        "#         dataset = prepare_dataset(dataset_name, dataset_path=\"./data/processed/\")\n",
        "#         dataset = dataset.set_index('id')\n",
        "\n",
        "#         fn = lambda sample: lead(sample, N=2*n)\n",
        "\n",
        "\n",
        "#         df = construct_templated_summaries(data, fn, dataset=dataset)\n",
        "\n",
        "#         df['metadata/method'] = \"Lead\"\n",
        "#         df['metadata/n_sentences'] = 2*n\n",
        "\n",
        "#         name = file.stem + \"-_-\" + f\"lead_{2*n}.csv\"\n",
        "\n",
        "#         if (output_path / name).exists():\n",
        "#             df_old = pd.read_csv(output_path / name)\n",
        "\n",
        "#             for col in df.columns:\n",
        "#                 if col not in df_old.columns:\n",
        "#                     df_old[col] = float(\"nan\")\n",
        "\n",
        "#             # add entry to the dataframe\n",
        "#             for col in df.columns:\n",
        "#                 df_old[col] = df[col]\n",
        "\n",
        "#             df = df_old\n",
        "\n",
        "#         df.to_csv(output_path / name)\n",
        "\n",
        "#         import seaborn as sns\n",
        "#         import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#         import subprocess\n",
        "\n",
        "\n",
        "# for file in output_path.glob(\"*.csv\"):\n",
        "#     print(\"evaluate_bartbert_metrixcs\")\n",
        "#     print(file)\n",
        "#     cmd = [\"python\", \"mds/evaluate_bartbert_metrics.py\", \"--summaries\", file]\n",
        "#     subprocess.run(cmd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dfs = []\n",
        "for file in output_path.glob(\"*.csv\"):\n",
        "    model_name, dataset_name, decoding_config, date = str(file.stem).split('-_-')[:4]\n",
        "    method =  str(file.stem).split('-_-')[-1]\n",
        "\n",
        "    df = pd.read_csv(file)\n",
        "    df['metadata/Model'] = model_name\n",
        "    df['metadata/Dataset'] = dataset_name\n",
        "    df['metadata/method'] = method\n",
        "\n",
        "    df[\"Method\"] = f\"{model_name}/{method}\"\n",
        "\n",
        "    dfs.append(df)\n",
        "    # print(\"dfs dfs dfs\")\n",
        "    # print(dfs)\n",
        "df = pd.concat(dfs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sns.catplot(data=df, hue='Method', y='rougeL', x='metadata/Dataset', kind='bar')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
